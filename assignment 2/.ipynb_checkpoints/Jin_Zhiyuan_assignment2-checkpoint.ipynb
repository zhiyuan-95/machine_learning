{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0f9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('8K_diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977281a",
   "metadata": {},
   "source": [
    "### 1. Perform Exploratory Data Analysis (EDA) and discuss the data and what you observe prior to beginning modeling and how impact how to proceed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe62c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   race                      8000 non-null   object\n",
      " 1   gender                    8000 non-null   object\n",
      " 2   age                       8000 non-null   object\n",
      " 3   weight                    8000 non-null   object\n",
      " 4   admission_type_id         7424 non-null   object\n",
      " 5   discharge_disposition_id  7627 non-null   object\n",
      " 6   admission_source_id       7250 non-null   object\n",
      " 7   time_in_hospital          8000 non-null   int64 \n",
      " 8   payer_code                8000 non-null   object\n",
      " 9   medical_specialty         8000 non-null   object\n",
      " 10  num_lab_procedures        8000 non-null   int64 \n",
      " 11  num_procedures            8000 non-null   int64 \n",
      " 12  num_medications           8000 non-null   int64 \n",
      " 13  number_outpatient         8000 non-null   int64 \n",
      " 14  number_emergency          8000 non-null   int64 \n",
      " 15  number_inpatient          8000 non-null   int64 \n",
      " 16  diag_1                    8000 non-null   object\n",
      " 17  diag_2                    8000 non-null   object\n",
      " 18  diag_3                    8000 non-null   object\n",
      " 19  number_diagnoses          8000 non-null   int64 \n",
      " 20  max_glu_serum             8000 non-null   object\n",
      " 21  A1Cresult                 8000 non-null   object\n",
      " 22  metformin                 8000 non-null   object\n",
      " 23  repaglinide               8000 non-null   object\n",
      " 24  nateglinide               8000 non-null   object\n",
      " 25  chlorpropamide            8000 non-null   object\n",
      " 26  glimepiride               8000 non-null   object\n",
      " 27  acetohexamide             8000 non-null   object\n",
      " 28  glipizide                 8000 non-null   object\n",
      " 29  glyburide                 8000 non-null   object\n",
      " 30  tolbutamide               8000 non-null   object\n",
      " 31  pioglitazone              8000 non-null   object\n",
      " 32  rosiglitazone             8000 non-null   object\n",
      " 33  acarbose                  8000 non-null   object\n",
      " 34  miglitol                  8000 non-null   object\n",
      " 35  troglitazone              8000 non-null   object\n",
      " 36  tolazamide                8000 non-null   object\n",
      " 37  examide                   8000 non-null   object\n",
      " 38  citoglipton               8000 non-null   object\n",
      " 39  insulin                   8000 non-null   object\n",
      " 40  glyburide.metformin       8000 non-null   object\n",
      " 41  glipizide.metformin       8000 non-null   object\n",
      " 42  glimepiride.pioglitazone  8000 non-null   object\n",
      " 43  metformin.rosiglitazone   8000 non-null   object\n",
      " 44  metformin.pioglitazone    8000 non-null   object\n",
      " 45  change                    8000 non-null   object\n",
      " 46  diabetesMed               8000 non-null   object\n",
      " 47  readmitted                8000 non-null   bool  \n",
      " 48  diag_1_desc               8000 non-null   object\n",
      " 49  diag_2_desc               7945 non-null   object\n",
      " 50  diag_3_desc               7825 non-null   object\n",
      "dtypes: bool(1), int64(8), object(42)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9f385",
   "metadata": {},
   "source": [
    "    this data set contains different types of data like text data, numberical data, catagorical data, and bool data. and some of the columns contains large amount missing values.\n",
    "    if there are some missing values that will impact the accuracy of the model, and if the majority of the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817b82f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the reason why I do this is to replace these two string value \"?\" and \"None\" to None to get the true number of missing value.\n",
    "for x in df.columns:\n",
    "    for y in range(8000):\n",
    "        i = df.loc[y,x]\n",
    "        if i=='?' or i=='None' or type(i)==float:\n",
    "            df.loc[y,x]= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baf95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419f2742",
   "metadata": {},
   "source": [
    "I want to seperate the original data data set to categorical, numerical, bool, and text data set to apply different preprocessing processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f0965",
   "metadata": {},
   "source": [
    "### 2. Pre-processed categorical data for use in the model and justified pre-processing method. Note this may be different for each algorithm you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e3c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data= df.columns[10:20]\n",
    "text_df = df.columns[-3:]\n",
    "insufficient_data = ['weight','payer_code','medical_specialty','max_glu_serum','A1Cresult']\n",
    "categorical_data = df.drop(columns = numerical_data)\n",
    "categorical_data = categorical_data.drop(columns = text_df)\n",
    "categorical_data = categorical_data.drop(columns = insufficient_data)\n",
    "# time in hospital is numerical data\n",
    "categorical_data = categorical_data.drop(columns = ['time_in_hospital'])\n",
    "useless = []\n",
    "for x in categorical_data.columns:\n",
    "    if len(categorical_data[x].value_counts())==1:\n",
    "        useless.append(x)\n",
    "\n",
    "cate_df = categorical_data.drop(columns = useless)\n",
    "cate_df = cate_df.drop(columns = ['readmitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413d4925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Caucasian          5891\n",
      "AfricanAmerican    1639\n",
      "Hispanic            146\n",
      "Other               101\n",
      "Asian                43\n",
      "Name: race, dtype: int64\n",
      "\n",
      "--\n",
      "Female    4314\n",
      "Male      3686\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "--\n",
      "[70-80)     2049\n",
      "[60-70)     1749\n",
      "[50-60)     1391\n",
      "[80-90)     1248\n",
      "[40-50)      800\n",
      "[30-40)      340\n",
      "[90-100)     214\n",
      "[20-30)      117\n",
      "[10-20)       65\n",
      "[0-10)        27\n",
      "Name: age, dtype: int64\n",
      "\n",
      "--\n",
      "Emergency        3968\n",
      "Urgent           1545\n",
      "Elective         1401\n",
      "Not Available     476\n",
      "Not Mapped         33\n",
      "Newborn             1\n",
      "Name: admission_type_id, dtype: int64\n",
      "\n",
      "--\n",
      "Discharged to home                                                                                             4879\n",
      "Discharged/transferred to SNF                                                                                   948\n",
      "Discharged/transferred to home with home health service                                                         912\n",
      "Discharged/transferred to another short term hospital                                                           156\n",
      "Expired                                                                                                         154\n",
      "Discharged/transferred to another rehab fac including rehab units of a hospital.                                140\n",
      "Discharged/transferred to another  type of inpatient care institution                                           111\n",
      "Not Mapped                                                                                                       99\n",
      "Discharged/transferred to ICF                                                                                    75\n",
      "Discharged/transferred to a long term care hospital.                                                             40\n",
      "Left AMA                                                                                                         33\n",
      "Hospice / home                                                                                                   29\n",
      "Hospice / medical facility                                                                                       24\n",
      "Discharged/transferred to home under care of Home IV provider                                                    12\n",
      "Discharged/transferred/referred to a psychiatric hospital of a psychiatric distinct part unit of a hospital       8\n",
      "Discharged/transferred within this institution to Medicare approved swing bed                                     2\n",
      "Admitted as an inpatient to this hospital                                                                         1\n",
      "Discharged/transferred/referred another institution for outpatient services                                       1\n",
      "Discharged/transferred to a federal health care facility.                                                         1\n",
      "Discharged/transferred/referred to this institution for outpatient services                                       1\n",
      "Discharged/transferred to a nursing facility certified under Medicaid but not certified under Medicare            1\n",
      "Name: discharge_disposition_id, dtype: int64\n",
      "\n",
      "--\n",
      "Emergency Room                                    3969\n",
      "Physician Referral                                2387\n",
      "Transfer from a hospital                           322\n",
      "Transfer from another health care facility         286\n",
      "Clinic Referral                                    134\n",
      "Transfer from a Skilled Nursing Facility (SNF)      93\n",
      "HMO Referral                                        29\n",
      "Not Mapped                                          16\n",
      "Not Available                                       13\n",
      "Court/Law Enforcement                                1\n",
      "Name: admission_source_id, dtype: int64\n",
      "\n",
      "--\n",
      "No        6409\n",
      "Steady    1460\n",
      "Up          90\n",
      "Down        41\n",
      "Name: metformin, dtype: int64\n",
      "\n",
      "--\n",
      "No        7888\n",
      "Steady      96\n",
      "Up          11\n",
      "Down         5\n",
      "Name: repaglinide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7962\n",
      "Steady      36\n",
      "Down         1\n",
      "Up           1\n",
      "Name: nateglinide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7990\n",
      "Steady       9\n",
      "Up           1\n",
      "Name: chlorpropamide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7604\n",
      "Steady     359\n",
      "Up          28\n",
      "Down         9\n",
      "Name: glimepiride, dtype: int64\n",
      "\n",
      "--\n",
      "No        6938\n",
      "Steady     946\n",
      "Up          76\n",
      "Down        40\n",
      "Name: glipizide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7029\n",
      "Steady     838\n",
      "Up          79\n",
      "Down        54\n",
      "Name: glyburide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7998\n",
      "Steady       2\n",
      "Name: tolbutamide, dtype: int64\n",
      "\n",
      "--\n",
      "No        7440\n",
      "Steady     523\n",
      "Up          24\n",
      "Down        13\n",
      "Name: pioglitazone, dtype: int64\n",
      "\n",
      "--\n",
      "No        7393\n",
      "Steady     587\n",
      "Up          11\n",
      "Down         9\n",
      "Name: rosiglitazone, dtype: int64\n",
      "\n",
      "--\n",
      "No        7976\n",
      "Steady      23\n",
      "Up           1\n",
      "Name: acarbose, dtype: int64\n",
      "\n",
      "--\n",
      "No        7997\n",
      "Steady       2\n",
      "Down         1\n",
      "Name: miglitol, dtype: int64\n",
      "\n",
      "--\n",
      "No        7999\n",
      "Steady       1\n",
      "Name: tolazamide, dtype: int64\n",
      "\n",
      "--\n",
      "No        4112\n",
      "Steady    2457\n",
      "Down       755\n",
      "Up         676\n",
      "Name: insulin, dtype: int64\n",
      "\n",
      "--\n",
      "No        7956\n",
      "Steady      42\n",
      "Down         1\n",
      "Up           1\n",
      "Name: glyburide.metformin, dtype: int64\n",
      "\n",
      "--\n",
      "No        7998\n",
      "Steady       2\n",
      "Name: glipizide.metformin, dtype: int64\n",
      "\n",
      "--\n",
      "No    4580\n",
      "Ch    3420\n",
      "Name: change, dtype: int64\n",
      "\n",
      "--\n",
      "Yes    6014\n",
      "No     1986\n",
      "Name: diabetesMed, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in cate_df.columns:\n",
    "    print('--')\n",
    "    print(cate_df[x].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79732fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   race                      7820 non-null   object\n",
      " 1   gender                    8000 non-null   object\n",
      " 2   age                       8000 non-null   object\n",
      " 3   admission_type_id         7424 non-null   object\n",
      " 4   discharge_disposition_id  7627 non-null   object\n",
      " 5   admission_source_id       7250 non-null   object\n",
      " 6   metformin                 8000 non-null   object\n",
      " 7   repaglinide               8000 non-null   object\n",
      " 8   nateglinide               8000 non-null   object\n",
      " 9   chlorpropamide            8000 non-null   object\n",
      " 10  glimepiride               8000 non-null   object\n",
      " 11  glipizide                 8000 non-null   object\n",
      " 12  glyburide                 8000 non-null   object\n",
      " 13  tolbutamide               8000 non-null   object\n",
      " 14  pioglitazone              8000 non-null   object\n",
      " 15  rosiglitazone             8000 non-null   object\n",
      " 16  acarbose                  8000 non-null   object\n",
      " 17  miglitol                  8000 non-null   object\n",
      " 18  tolazamide                8000 non-null   object\n",
      " 19  insulin                   8000 non-null   object\n",
      " 20  glyburide.metformin       8000 non-null   object\n",
      " 21  glipizide.metformin       8000 non-null   object\n",
      " 22  change                    8000 non-null   object\n",
      " 23  diabetesMed               8000 non-null   object\n",
      "dtypes: object(24)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "cate_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a88fb",
   "metadata": {},
   "source": [
    "        in order to process the decision tree or random forest, we have to transform it to binary value by using one hot encode, so that we can implement the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cd86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1a0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehotencoder with deal with the missing value\n",
    "cate_encoder = OneHotEncoder()\n",
    "cate_df_1hot = cate_encoder.fit_transform(cate_df)\n",
    "one_hot_feature = cate_encoder.get_feature_names_out()\n",
    "cate_np = cate_df_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7f820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['race_AfricanAmerican', 'race_Asian', 'race_Caucasian',\n",
       "       'race_Hispanic', 'race_Other', 'race_None', 'gender_Female',\n",
       "       'gender_Male', 'age_[0-10)', 'age_[10-20)', 'age_[20-30)',\n",
       "       'age_[30-40)', 'age_[40-50)', 'age_[50-60)', 'age_[60-70)',\n",
       "       'age_[70-80)', 'age_[80-90)', 'age_[90-100)',\n",
       "       'admission_type_id_Elective', 'admission_type_id_Emergency',\n",
       "       'admission_type_id_Newborn', 'admission_type_id_Not Available',\n",
       "       'admission_type_id_Not Mapped', 'admission_type_id_Urgent',\n",
       "       'admission_type_id_None',\n",
       "       'discharge_disposition_id_Admitted as an inpatient to this hospital',\n",
       "       'discharge_disposition_id_Discharged to home',\n",
       "       'discharge_disposition_id_Discharged/transferred to ICF',\n",
       "       'discharge_disposition_id_Discharged/transferred to SNF',\n",
       "       'discharge_disposition_id_Discharged/transferred to a federal health care facility.',\n",
       "       'discharge_disposition_id_Discharged/transferred to a long term care hospital.',\n",
       "       'discharge_disposition_id_Discharged/transferred to a nursing facility certified under Medicaid but not certified under Medicare',\n",
       "       'discharge_disposition_id_Discharged/transferred to another  type of inpatient care institution',\n",
       "       'discharge_disposition_id_Discharged/transferred to another rehab fac including rehab units of a hospital.',\n",
       "       'discharge_disposition_id_Discharged/transferred to another short term hospital',\n",
       "       'discharge_disposition_id_Discharged/transferred to home under care of Home IV provider',\n",
       "       'discharge_disposition_id_Discharged/transferred to home with home health service',\n",
       "       'discharge_disposition_id_Discharged/transferred within this institution to Medicare approved swing bed',\n",
       "       'discharge_disposition_id_Discharged/transferred/referred another institution for outpatient services',\n",
       "       'discharge_disposition_id_Discharged/transferred/referred to a psychiatric hospital of a psychiatric distinct part unit of a hospital',\n",
       "       'discharge_disposition_id_Discharged/transferred/referred to this institution for outpatient services',\n",
       "       'discharge_disposition_id_Expired',\n",
       "       'discharge_disposition_id_Hospice / home',\n",
       "       'discharge_disposition_id_Hospice / medical facility',\n",
       "       'discharge_disposition_id_Left AMA',\n",
       "       'discharge_disposition_id_Not Mapped',\n",
       "       'discharge_disposition_id_None',\n",
       "       'admission_source_id_Clinic Referral',\n",
       "       'admission_source_id_Court/Law Enforcement',\n",
       "       'admission_source_id_Emergency Room',\n",
       "       'admission_source_id_HMO Referral',\n",
       "       'admission_source_id_Not Available',\n",
       "       'admission_source_id_Not Mapped',\n",
       "       'admission_source_id_Physician Referral',\n",
       "       'admission_source_id_Transfer from a Skilled Nursing Facility (SNF)',\n",
       "       'admission_source_id_Transfer from a hospital',\n",
       "       'admission_source_id_Transfer from another health care facility',\n",
       "       'admission_source_id_None', 'metformin_Down', 'metformin_No',\n",
       "       'metformin_Steady', 'metformin_Up', 'repaglinide_Down',\n",
       "       'repaglinide_No', 'repaglinide_Steady', 'repaglinide_Up',\n",
       "       'nateglinide_Down', 'nateglinide_No', 'nateglinide_Steady',\n",
       "       'nateglinide_Up', 'chlorpropamide_No', 'chlorpropamide_Steady',\n",
       "       'chlorpropamide_Up', 'glimepiride_Down', 'glimepiride_No',\n",
       "       'glimepiride_Steady', 'glimepiride_Up', 'glipizide_Down',\n",
       "       'glipizide_No', 'glipizide_Steady', 'glipizide_Up',\n",
       "       'glyburide_Down', 'glyburide_No', 'glyburide_Steady',\n",
       "       'glyburide_Up', 'tolbutamide_No', 'tolbutamide_Steady',\n",
       "       'pioglitazone_Down', 'pioglitazone_No', 'pioglitazone_Steady',\n",
       "       'pioglitazone_Up', 'rosiglitazone_Down', 'rosiglitazone_No',\n",
       "       'rosiglitazone_Steady', 'rosiglitazone_Up', 'acarbose_No',\n",
       "       'acarbose_Steady', 'acarbose_Up', 'miglitol_Down', 'miglitol_No',\n",
       "       'miglitol_Steady', 'tolazamide_No', 'tolazamide_Steady',\n",
       "       'insulin_Down', 'insulin_No', 'insulin_Steady', 'insulin_Up',\n",
       "       'glyburide.metformin_Down', 'glyburide.metformin_No',\n",
       "       'glyburide.metformin_Steady', 'glyburide.metformin_Up',\n",
       "       'glipizide.metformin_No', 'glipizide.metformin_Steady',\n",
       "       'change_Ch', 'change_No', 'diabetesMed_No', 'diabetesMed_Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97bad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 117)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cate_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4557dd6",
   "metadata": {},
   "source": [
    "### 3. Pre-processed numerical data appropriately including handling missing data and justified methods used. Note this may be different for each algorithm you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical data set\n",
    "numerical_data = list(numerical_data)\n",
    "numerical_data.append('time_in_hospital')\n",
    "num_df = df[numerical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cda1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   num_lab_procedures  8000 non-null   int64 \n",
      " 1   num_procedures      8000 non-null   int64 \n",
      " 2   num_medications     8000 non-null   int64 \n",
      " 3   number_outpatient   8000 non-null   int64 \n",
      " 4   number_emergency    8000 non-null   int64 \n",
      " 5   number_inpatient    8000 non-null   int64 \n",
      " 6   diag_1              8000 non-null   object\n",
      " 7   diag_2              7945 non-null   object\n",
      " 8   diag_3              7825 non-null   object\n",
      " 9   number_diagnoses    8000 non-null   int64 \n",
      " 10  time_in_hospital    8000 non-null   int64 \n",
      "dtypes: int64(8), object(3)\n",
      "memory usage: 687.6+ KB\n"
     ]
    }
   ],
   "source": [
    "num_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f691512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = ['diag_1','diag_2','diag_3']\n",
    "\n",
    "for i in diags:\n",
    "    for x in range(8000):\n",
    "        a = num_df[i].iloc[x]\n",
    "        if a != None:\n",
    "            if a[0]=='V'or a[0]=='E':\n",
    "                num_df.loc[x,i]= None\n",
    "            else:\n",
    "                num_df.loc[x,i] = float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fff1b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\3907522196.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_df[x] = num_df[x].fillna(num_df[x].median())\n",
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\3907522196.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_df[x] = num_df[x].fillna(num_df[x].median())\n",
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\3907522196.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_df[x] = num_df[x].fillna(num_df[x].median())\n"
     ]
    }
   ],
   "source": [
    "for x in diags:\n",
    "    num_df[x] = num_df[x].fillna(num_df[x].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0972374",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_np = np.array(num_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea55e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a80a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716cb33",
   "metadata": {},
   "source": [
    "#### 4. Implement a model to make predictions using text data using tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98175fdb",
   "metadata": {},
   "source": [
    "    tf(t,d) = count of t in d / number of words in d\n",
    "    df(t) = occurrence of t in N documents, DF is the number of documents in which the word is present at least once.\n",
    "    N/df = this means how many docs are we expected to read to have one occurance of term t.\n",
    "    idf(t) = log(N/(df + 1)), in case we will see some case df = 0, we add 1 on df.\n",
    "    tf-idf(t, d) = tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b131217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided to use random string to fill missing value, because maybe the reason why there is no desc is because the patient is cured\n",
    "df['diag_2_desc'] = df['diag_2_desc'].fillna('jin')\n",
    "df['diag_3_desc'] = df['diag_3_desc'].fillna('zhiyuan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c3bdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea3d3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stops(v1, v2, top_n=30):\n",
    "    i = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    for v in [v1, v2]:\n",
    "        vectorizer.fit_transform(v)\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "\n",
    "        numb_of_appearance_true = {}\n",
    "        for x in features:\n",
    "            numb_of_appearance_true[x]=0\n",
    "        for x in features:\n",
    "            for y in v:\n",
    "                if x in y.lower():\n",
    "                    numb_of_appearance_true[x]+=1    \n",
    "        i.append(sorted(numb_of_appearance_true.items(), key=lambda x:x[1], reverse = True))\n",
    "    a = [x[0] for x in i[0][:top_n]]\n",
    "    b = [x[0] for x in i[1][:top_n]]\n",
    "    stop = []\n",
    "    for x in a:\n",
    "        if x in b:\n",
    "            stop.append(x)\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fe52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d4b7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_score(DF,column_index,top_n):\n",
    "    # I split the column into two parts which are readmitted = true, and readmitted = false\n",
    "    # so that we can find out how each word are related with these to type of documents\n",
    "    part_true = df.loc[df.iloc[:,-4]==True]\n",
    "    part_false = df.loc[df.iloc[:,-4]==False]\n",
    "    stopWords = get_stops(part_true,part_false, top_n)\n",
    "\n",
    "    part_true = part_true.iloc[:,column_index]\n",
    "    part_false = part_false.iloc[:,column_index]\n",
    "    d_true = ''\n",
    "    for x in part_true:\n",
    "        d_true = d_true+' '+x\n",
    "    d_false = ''\n",
    "    for x in part_false:\n",
    "        d_false = d_false+' '+x\n",
    "    corpus = [d_true, d_false]\n",
    "\n",
    "    vectorizer_ = TfidfVectorizer(stop_words = stopWords)\n",
    "    X = vectorizer_.fit_transform(corpus)\n",
    "    toker = vectorizer_.build_tokenizer()\n",
    "    features = vectorizer_.get_feature_names_out()\n",
    "    vocabulary = vectorizer_.vocabulary_\n",
    "\n",
    "    # X is two arrays of tfidf score of each feature in these two document.\n",
    "    # this is the score for each feature    \n",
    "\n",
    "    # X.toarray looks like following. the first row is the relativity bewteen each word and doc 1. the second is ....doc 2. \n",
    "    # [[0.02490878 0.00373632 0.00124544 ... 0.01245439 0.01245439 0.00871807]\n",
    "    # [0.01528978 0.00849432 0.00679546 ... 0.00509659 0.01019319 0.00169886]]\n",
    "    # by subtracting them I have have the words more relate to doc1 having positive score, and the words more realte to doc2 nagative score.\n",
    "    feature_score = 100*(X.toarray()[0]-X.toarray()[1])    \n",
    "    return vectorizer_, feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b16fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_score_calculator(DF,column_index,vectorizer_, feature_score):\n",
    "    toker = vectorizer_.build_tokenizer()\n",
    "    features = vectorizer_.get_feature_names_out()\n",
    "    vocabulary = vectorizer_.vocabulary_\n",
    "    scores_d1 = []\n",
    "    for x in DF.iloc[:,column_index]:\n",
    "        sample_score = 0\n",
    "        x = x.lower()\n",
    "        for y in toker(x):\n",
    "            if y in features:\n",
    "                index = vocabulary[y]\n",
    "                sample_score+=feature_score[index]\n",
    "        scores_d1.append(sample_score)\n",
    "    return scores_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3a3c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(feature_matrix, target_vector, t, shuffle = True):\n",
    "    np.random.seed(42)\n",
    "    test_set_size = int(len(feature_matrix)*t)\n",
    "    if shuffle ==True:\n",
    "        shuffled_indices = np.random.permutation(len(feature_matrix))\n",
    "        test_indices = shuffled_indices[:test_set_size]\n",
    "        train_indices = shuffled_indices[test_set_size:]\n",
    "        return feature_matrix[train_indices], feature_matrix[test_indices], target_vector[train_indices], target_vector[test_indices]\n",
    "    else:\n",
    "        train_set_size = len(feature_matrix)-test_set_size\n",
    "        return feature_matrix[train_indices], feature_matrix[test_indices], target_vector[train_indices], target_vector[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dbf9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def sFold_text(folds, data, labels, error_function, model, **model_args):\n",
    "    kf = KFold(n_splits=folds, random_state=None, shuffle = True)\n",
    "    scores = []\n",
    "    y = labels\n",
    "    if model == LogisticRegression:\n",
    "        M = model(max_iter = 10**9)\n",
    "    if model == GradientBoostingClassifier:\n",
    "        M=model()\n",
    "    if model == RandomForestClassifier:\n",
    "        M=model()\n",
    "    if model == AdaBoostClassifier:\n",
    "        if len(model_args)!=0:\n",
    "            M=model(learning_rate = model_args['learning_rate'])\n",
    "        else:\n",
    "            M = model()\n",
    "    if model == SVC:\n",
    "        M=model(probability = True)\n",
    "    if model == GaussianNB:\n",
    "        M =model()\n",
    "    if model == KNeighborsClassifier:\n",
    "        M = model()\n",
    "    for train_index,test_index in kf.split(data):        \n",
    "        x_training_set = data.iloc[train_index]\n",
    "        y_training_set = y.iloc[train_index]\n",
    "    # get term score from the training\n",
    "        v_d1,ts_d1 = term_score(x_training_set, -3,36)\n",
    "        v_d2,ts_d2 = term_score(x_training_set, -2,36)\n",
    "        v_d3,ts_d3 = term_score(x_training_set, -1,36)\n",
    "    # find out the score for text by term score for each text column.\n",
    "        text_score_d1 = text_score_calculator(x_training_set, -3,v_d1,ts_d1)\n",
    "        text_score_d2 = text_score_calculator(x_training_set, -2,v_d2,ts_d2)\n",
    "        text_score_d3 = text_score_calculator(x_training_set, -1,v_d3,ts_d3)\n",
    "        text_train = np.array([text_score_d1,text_score_d2,text_score_d3])\n",
    "    # transformed training set\n",
    "        x_training_set = np.transpose(text_train)\n",
    "        x_test_set = data.iloc[test_index]\n",
    "    # use the term score output from training set to get the \n",
    "    # score for the score for each text data for test set.\n",
    "        text_score_d1_test = text_score_calculator(x_test_set, -3, v_d1, ts_d1)\n",
    "        text_score_d2_test = text_score_calculator(x_test_set, -2, v_d2, ts_d2)\n",
    "        text_score_d3_test = text_score_calculator(x_test_set, -1, v_d3, ts_d3)\n",
    "        text_test = np.array([text_score_d1_test,text_score_d2_test,text_score_d3_test])\n",
    "    # new test set\n",
    "        x_test_set = np.transpose(text_test)    \n",
    "        y_test_set = y.iloc[test_index]\n",
    "        y_training_set = [x for x in np.array(y_training_set)]\n",
    "        \n",
    "        M.fit(np.array(x_training_set), y_training_set)\n",
    "        y_pred = M.predict_proba(x_test_set)\n",
    "        y_pred = [x[1] for x in y_pred]\n",
    "        score = error_function(y_test_set,y_pred)\n",
    "        scores.append(score)\n",
    "        print('.',end='')\n",
    "    average_error = round(sum(scores)/folds,4)\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bb5f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['readmitted'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05fe76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d1,ts_d1 = term_score(df, -3,36)\n",
    "v_d2,ts_d2 = term_score(df, -2,36)\n",
    "v_d3,ts_d3 = term_score(df, -1,36)\n",
    "# find out the score for text by term score for each text column.\n",
    "text_score_d1 = text_score_calculator(df, -3,v_d1,ts_d1)\n",
    "text_score_d2 = text_score_calculator(df, -2,v_d2,ts_d2)\n",
    "text_score_d3 = text_score_calculator(df, -1,v_d3,ts_d3)\n",
    "text_np = np.array([text_score_d1,text_score_d2,text_score_d3]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eeaa90e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5955"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = sFold_text(5, df, target, roc_auc_score , AdaBoostClassifier)\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca81e9",
   "metadata": {},
   "source": [
    "### 5. Use model stacking to incorporate tf-idf predictions for all 3 text fields (so 3 models unless you elect to concatenate the text fields into 1 - need to justify if so) in downstream algorithm the uses non-text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29e9734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function \n",
    "def level0_text_predictor(t_train,y, model):\n",
    "    leng = int(len(t_train)/2)\n",
    "    t_a, t_b= t_train[:leng], t_train[leng:]\n",
    "    tg_a, tg_b = y[:leng], y[leng:]\n",
    "    m1 = model()\n",
    "    m1.fit(t_a,tg_a)\n",
    "    tp_b = [i[1] for i in m1.predict_proba(t_b)]\n",
    "\n",
    "    m2 = model()\n",
    "    m2.fit(t_b,tg_b)\n",
    "    tp_a = [i[1] for i in m2.predict_proba(t_a)]\n",
    "    \n",
    "    tp = [[x] for x in np.hstack((tp_a,tp_b))]\n",
    "    \n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46f5807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking(input_x,y,model):\n",
    "    t1,t2,t3 = [[x] for x in input_x[:,-3]],[[x] for x in input_x[:,-2]],[[x] for x in input_x[:,-1]]\n",
    "    x = np.delete(input_x,[-1,-2,-3],1)\n",
    "    y = np.array(y).astype(float)\n",
    "    t1_meta = level0_text_predictor(t1,y,model)\n",
    "    t2_meta = level0_text_predictor(t2,y,model)\n",
    "    t3_meta = level0_text_predictor(t3,y,model)\n",
    "    output_x = np.concatenate((x,t1_meta,t2_meta, t3_meta),axis = 1)\n",
    "    m_3 = []\n",
    "    # m_3 is the output model for the new data set which fed with the entire training set\n",
    "    for t in [t1,t2,t3]:\n",
    "        m3 = model()\n",
    "        m3.fit(t,y)\n",
    "        m_3.append(m3)\n",
    "    return output_x,m_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34b5ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to transform the outsample text data to leval0 prediction\n",
    "# which is predicted by the output model from the function train_stacking\n",
    "def test_stacking(input_x,model):\n",
    "    t1,t2,t3 = [[x] for x in input_x[:,-3]],[[x] for x in input_x[:,-2]],[[x] for x in input_x[:,-1]]\n",
    "    x = np.delete(input_x,[-1,-2,-3],1)\n",
    "    t1_meta = [[x[1]] for x in model[0].predict_proba(t1)]\n",
    "    t2_meta = [[x[1]] for x in model[1].predict_proba(t2)]\n",
    "    t3_meta = [[x[1]] for x in model[2].predict_proba(t3)]\n",
    "    output_x = np.concatenate((x,t1_meta,t2_meta, t3_meta),axis = 1)\n",
    "    return output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed1f9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacking:\n",
    "    def __init__(self, m1,m2,**m2_args):\n",
    "        self._level1_predictor = None\n",
    "        self._model_for_text_stacking = m1\n",
    "        if m2==LogisticRegression:\n",
    "            self._model_for_meta = m2(max_iter = 10**5)\n",
    "        elif m2==GradientBoostingClassifier:\n",
    "            if len(m2_args)==0:\n",
    "                self._model_for_meta = m2()\n",
    "            else:\n",
    "                self._model_for_meta = m2(learning_rate = m2_args['learning_rate'],max_depth=m2_args['max_depth'])\n",
    "        else:\n",
    "            self._model_for_meta = m2()\n",
    "    def fit(self, X_train, Y_train):\n",
    "        output = train_stacking(X_train,Y_train,self._model_for_text_stacking)\n",
    "        self._level1_predictor = output[1]\n",
    "        self._model_for_meta.fit(output[0],Y_train)\n",
    "    def predict_proba(self, new_data):\n",
    "        new_data_meta = test_stacking(new_data,self._level1_predictor)\n",
    "        target_pred_proba = self._model_for_meta.predict_proba(new_data_meta)\n",
    "        target_pred_proba = [i[1] for i in target_pred_proba]\n",
    "        return target_pred_proba\n",
    "    def predict(self, new_data):\n",
    "        new_data_meta = test_stacking(new_data,self._level1_predictor)\n",
    "        target_pred = self._model_for_meta.predict(new_data_meta)\n",
    "        return target_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f27f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def sFold_stacking(folds, data, labels, error_function, model1,model2, **model_args):\n",
    "    kf = KFold(n_splits=folds, random_state=None, shuffle = True)\n",
    "    scores = []\n",
    "    y = pd.DataFrame(labels)\n",
    "    data = pd.DataFrame(data)\n",
    "    M = stacking(model1, model2,**model_args)\n",
    "    for train_index,test_index in kf.split(data):\n",
    "        x_training_set = data.iloc[train_index]\n",
    "        y_training_set = y.iloc[train_index]\n",
    "        x_test_set = data.iloc[test_index]\n",
    "        y_test_set = y.iloc[test_index]\n",
    "        y_training_set = [x[0] for x in np.array(y_training_set)]\n",
    "        M.fit(np.array(x_training_set), y_training_set)\n",
    "        y_pred = M.predict_proba(np.array(x_test_set))\n",
    "        score = error_function(y_test_set,y_pred)\n",
    "        scores.append(score)\n",
    "        print('.',end='')\n",
    "    average_error = round(sum(scores)/folds,4)\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84d59aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 117)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cec84a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "454b4855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57c9e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = np.concatenate((cate_np,num_np, text_np),axis = 1)\n",
    "x_train, x_test, y_train, y_test = partition(con, df['readmitted'],0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72a94579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 117)\n",
      "(8000, 11)\n",
      "(8000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(cate_np))\n",
    "print(np.shape(num_np))\n",
    "print(np.shape(text_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b934d396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044660409768078"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ['newton-cg','lbfgs']\n",
    "\n",
    "i = stacking(AdaBoostClassifier,LogisticRegression, penalty = 'l2', solver ='saga')\n",
    "i.fit(x_train, y_train)\n",
    "j = i.predict_proba(x_test)\n",
    "roc_auc_score(y_test,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a162e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6762"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sFold_stacking(5,x_train, y_train,roc_auc_score,AdaBoostClassifier,LogisticRegression,learning_rate = 0.1,max_depth = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07a724",
   "metadata": {},
   "source": [
    "### 6. Perform experimentation for multiple modeling algorithms and justify why you selected the experiments you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c69d7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(folds, data, labels, error_function, model, **model_args):\n",
    "    kf = KFold(n_splits=folds, random_state=None, shuffle = True)\n",
    "    scores = []\n",
    "    y = labels\n",
    "    data = pd.DataFrame(data)\n",
    "    if model == LogisticRegression:\n",
    "        M = model(max_iter = 10**9)\n",
    "    if model == GradientBoostingClassifier:\n",
    "        M=model()\n",
    "    if model == RandomForestClassifier:\n",
    "        M=model()\n",
    "    if model == AdaBoostClassifier:\n",
    "        if len(model_args)!=0:\n",
    "            M=model(learning_rate = model_args['learning_rate'])\n",
    "        else:\n",
    "            M = model()\n",
    "    if model == SVC:\n",
    "        M=model(probability = True)\n",
    "    if model == GaussianNB:\n",
    "        M =model()\n",
    "    if model == KNeighborsClassifier:\n",
    "        M = model()\n",
    "    for train_index,test_index in kf.split(data):        \n",
    "        x_training_set = data.iloc[train_index]\n",
    "        y_training_set = y.iloc[train_index]\n",
    "\n",
    "        x_test_set = data.iloc[test_index] \n",
    "        y_test_set = y.iloc[test_index]\n",
    "        y_training_set = [x for x in np.array(y_training_set)]\n",
    "        \n",
    "        M.fit(np.array(x_training_set), y_training_set)\n",
    "        y_pred = M.predict_proba(x_test_set)\n",
    "        y_pred = [x[1] for x in y_pred]\n",
    "        score = error_function(y_test_set,y_pred)\n",
    "        scores.append(score)\n",
    "        print('.',end='')\n",
    "    average_error = round(sum(scores)/folds,4)\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91aa59",
   "metadata": {},
   "source": [
    "I want to know the accuracy of this model without stacking. and the importance of each type of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97f07185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       1.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "7995    0.0\n",
       "7996    0.0\n",
       "7997    0.0\n",
       "7998    0.0\n",
       "7999    1.0\n",
       "Name: readmitted, Length: 8000, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e91f54d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6872"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = np.concatenate((cate_np,num_np),axis = 1)\n",
    "\n",
    "sFold(5, con,target, roc_auc_score,GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e65f6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6856"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con2 = np.concatenate((cate_np,text_np),axis = 1)\n",
    "\n",
    "sFold(5,  con,target, roc_auc_score,GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f01075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6863"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con3 = np.concatenate((num_np,text_np),axis = 1)\n",
    "sFold(5,  con,target, roc_auc_score,GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "26117020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6918"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con4 = np.concatenate((cate_np,num_np,text_np),axis = 1)\n",
    "sFold(5,  con,target, roc_auc_score,GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad7dd1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c839374",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [GradientBoostingClassifier, AdaBoostClassifier,LogisticRegression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9ff99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = partition(text_np, df['readmitted'],0.2)\n",
    "s1 = sFold(5, x_train, y_train, roc_auc_score , LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "495c02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....GradientBoostingClassifier 0.5972\n",
      ".....RandomForestClassifier 0.5571\n",
      ".....AdaBoostClassifier 0.6009\n",
      ".....LogisticRegression 0.5853\n",
      ".....GaussianNB 0.5861\n",
      ".....KNeighborsClassifier 0.5368\n"
     ]
    }
   ],
   "source": [
    "for x in models:\n",
    "    s1 = sFold(5, x_train, y_train, roc_auc_score , x)\n",
    "    print(x.__name__, s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9bfdd",
   "metadata": {},
   "source": [
    "adaboost preforms the best for the level 1 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c0bd7d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....1 0.5794\n",
      ".....2 0.5881\n",
      ".....3 0.5904\n",
      ".....4 0.5937\n",
      ".....5 0.5958\n",
      ".....6 0.5955\n",
      ".....7 0.5957\n",
      ".....8 0.5956\n",
      ".....9 0.5943\n",
      ".....10 0.5946\n",
      ".....11 0.5965\n",
      ".....12 0.5937\n",
      ".....13 0.5962\n",
      ".....14 0.5921\n",
      ".....15 0.5945\n",
      ".....16 0.5943\n",
      ".....17 0.5924\n",
      ".....18 0.5929\n",
      ".....19 0.5955\n"
     ]
    }
   ],
   "source": [
    "SCORE = []\n",
    "for s in range(1,20):\n",
    "    i = sFold(5, x_train, y_train, roc_auc_score,AdaBoostClassifier,learning_rate =0.01*s)\n",
    "    print(s,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb354a95",
   "metadata": {},
   "source": [
    "adaboost preforms best while the learning rate is 0.10, so that I changed the function level_0_text_predictor(). like following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level0_text_predictor(t_train,y, model):\n",
    "    leng = int(len(t_train)/2)\n",
    "    t_a, t_b= t_train[:leng], t_train[leng:]\n",
    "    tg_a, tg_b = y[:leng], y[leng:]\n",
    "    if model==AdaBoostClassifier:\n",
    "        m1 = model(learning_rate = 0.9)\n",
    "        m2 = model(learning_rate = 0.9)\n",
    "\n",
    "    m1.fit(t_a,tg_a)\n",
    "    tp_b = [i[1] for i in m1.predict_proba(t_b)]\n",
    "    m2.fit(t_b,tg_b)\n",
    "    tp_a = [i[1] for i in m2.predict_proba(t_a)]\n",
    "    \n",
    "    tp = [[x] for x in np.hstack((tp_a,tp_b))]\n",
    "    \n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = np.concatenate((cate_np,num_np, text_np),axis = 1)\n",
    "x_train, x_test, y_train, y_test = partition(con, df['readmitted'],0.2)\n",
    "for x in models:\n",
    "    i = sFold_stacking(5, x_train, y_train, roc_auc_score,AdaBoostClassifier, x)\n",
    "    print(i,x.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bfa418",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier preforms the best for the level 2 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765266d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(2,5):\n",
    "    for x in range(1,20):\n",
    "        i = sFold_stacking(5, x_train, y_train, roc_auc_score,AdaBoostClassifier, GradientBoostingClassifier,learning_rate = 0.02*x, max_depth = y)\n",
    "        print(i,x*0.02,y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b18d93",
   "metadata": {},
   "source": [
    "this experiment illustrated for the GradientBoostingClassifier in the range of 0.04<learning_rate<0.26, max depth = 2 and 0.04<learning_rate <0.2, max_depth =3 perfoms the best. so I kept as default for gradientboosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b489a",
   "metadata": {},
   "source": [
    "#### 7. Final model selection and discussion of your model choice and the model weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670f9b6",
   "metadata": {},
   "source": [
    "my selection of the model is Adaboost for the level 1 prediction, and gradientboost for level 2 prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872797d",
   "metadata": {},
   "source": [
    "the weaknee of this model is the model stacking didn't really increase the accuracy of the overall model. it is better of keep the text_np as it is than do the stacking for the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30b48e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....0.6848 GradientBoostingClassifier GradientBoostingClassifier\n",
      ".....0.6833 AdaBoostClassifier GradientBoostingClassifier\n",
      ".....0.6789 LogisticRegression GradientBoostingClassifier\n",
      ".....0.6767 GradientBoostingClassifier AdaBoostClassifier\n",
      ".....0.6786 AdaBoostClassifier AdaBoostClassifier\n",
      ".....0.6767 LogisticRegression AdaBoostClassifier\n",
      ".....0.6876 GradientBoostingClassifier LogisticRegression\n",
      ".....0.6777 AdaBoostClassifier LogisticRegression\n",
      ".....0.6822 LogisticRegression LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "for x in models:\n",
    "    for y in models:\n",
    "        SCORE = sFold_stacking(5,x_train,y_train, roc_auc_score,y,x)\n",
    "        print(SCORE, y.__name__, x.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c7d8d",
   "metadata": {},
   "source": [
    "0.6872 GradientBoostingClassifier LogisticRegression\n",
    "0.6849 GradientBoostingClassifier LogisticRegression\n",
    "0.6853 GradientBoostingClassifier LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af4ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682249d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c5e0009",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa8074",
   "metadata": {},
   "source": [
    "### part B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9b5822b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('2K_diabetes_scoring.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "16c1dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_t.columns:\n",
    "    for y in range(2000):\n",
    "        i = df_t.loc[y,x]\n",
    "        if i=='?' or i=='None' or type(i)==float:\n",
    "            df_t.loc[y,x]= None\n",
    "numerical_test= df_t.columns[10:20]\n",
    "categorical_test = df_t.drop(columns = numerical_data)\n",
    "categorical_test = categorical_test.drop(columns = text_df)\n",
    "categorical_test = categorical_test.drop(columns = insufficient_data)\n",
    "\n",
    "cate_test = categorical_test.drop(columns = useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b575caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   race                      7820 non-null   object\n",
      " 1   gender                    8000 non-null   object\n",
      " 2   age                       8000 non-null   object\n",
      " 3   admission_type_id         7424 non-null   object\n",
      " 4   discharge_disposition_id  7627 non-null   object\n",
      " 5   admission_source_id       7250 non-null   object\n",
      " 6   metformin                 8000 non-null   object\n",
      " 7   repaglinide               8000 non-null   object\n",
      " 8   nateglinide               8000 non-null   object\n",
      " 9   chlorpropamide            8000 non-null   object\n",
      " 10  glimepiride               8000 non-null   object\n",
      " 11  glipizide                 8000 non-null   object\n",
      " 12  glyburide                 8000 non-null   object\n",
      " 13  tolbutamide               8000 non-null   object\n",
      " 14  pioglitazone              8000 non-null   object\n",
      " 15  rosiglitazone             8000 non-null   object\n",
      " 16  acarbose                  8000 non-null   object\n",
      " 17  miglitol                  8000 non-null   object\n",
      " 18  tolazamide                8000 non-null   object\n",
      " 19  insulin                   8000 non-null   object\n",
      " 20  glyburide.metformin       8000 non-null   object\n",
      " 21  glipizide.metformin       8000 non-null   object\n",
      " 22  change                    8000 non-null   object\n",
      " 23  diabetesMed               8000 non-null   object\n",
      "dtypes: object(24)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "cate_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d4c9413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   race                      1959 non-null   object\n",
      " 1   gender                    2000 non-null   object\n",
      " 2   age                       2000 non-null   object\n",
      " 3   admission_type_id         1855 non-null   object\n",
      " 4   discharge_disposition_id  1904 non-null   object\n",
      " 5   admission_source_id       1814 non-null   object\n",
      " 6   metformin                 2000 non-null   object\n",
      " 7   repaglinide               2000 non-null   object\n",
      " 8   nateglinide               2000 non-null   object\n",
      " 9   chlorpropamide            2000 non-null   object\n",
      " 10  glimepiride               2000 non-null   object\n",
      " 11  glipizide                 2000 non-null   object\n",
      " 12  glyburide                 2000 non-null   object\n",
      " 13  tolbutamide               2000 non-null   object\n",
      " 14  pioglitazone              2000 non-null   object\n",
      " 15  rosiglitazone             2000 non-null   object\n",
      " 16  acarbose                  2000 non-null   object\n",
      " 17  miglitol                  2000 non-null   object\n",
      " 18  tolazamide                2000 non-null   object\n",
      " 19  insulin                   2000 non-null   object\n",
      " 20  glyburide.metformin       2000 non-null   object\n",
      " 21  glipizide.metformin       2000 non-null   object\n",
      " 22  change                    2000 non-null   object\n",
      " 23  diabetesMed               2000 non-null   object\n",
      "dtypes: object(24)\n",
      "memory usage: 375.1+ KB\n"
     ]
    }
   ],
   "source": [
    "cate_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dca6960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cate = pd.concat([cate_df,cate_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bd73843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_encoder_t = OneHotEncoder()\n",
    "cate_df_1hot = cate_encoder_t.fit_transform(temp_cate)\n",
    "temp_cate_np = cate_df_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "768baee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_test = temp_cate_np[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5dee25f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 118)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "700fd5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_lab_procedures', 'num_procedures', 'num_medications',\n",
       "       'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1',\n",
       "       'diag_2', 'diag_3', 'number_diagnoses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "17598c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_test = list(numerical_test)\n",
    "numerical_test.append('time_in_hospital')\n",
    "num_test = df_t[numerical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90f67931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>time_in_hospital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>401</td>\n",
       "      <td>244</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>411</td>\n",
       "      <td>284</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>401</td>\n",
       "      <td>250</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>552</td>\n",
       "      <td>682</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>433</td>\n",
       "      <td>401</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "      <td>401</td>\n",
       "      <td>272</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>250.02</td>\n",
       "      <td>V53</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>585</td>\n",
       "      <td>536</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>535</td>\n",
       "      <td>276</td>\n",
       "      <td>599</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>403</td>\n",
       "      <td>427</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_lab_procedures  num_procedures  num_medications  number_outpatient  \\\n",
       "0                      1               0               10                  0   \n",
       "1                     21               0               15                  1   \n",
       "2                     37               0               19                  1   \n",
       "3                     32               2                7                  0   \n",
       "4                     37               0               14                  0   \n",
       "...                  ...             ...              ...                ...   \n",
       "1995                  30               1               29                  0   \n",
       "1996                   1               5               15                  0   \n",
       "1997                  46               2               14                  0   \n",
       "1998                  62               1                7                  0   \n",
       "1999                  61               0               18                  0   \n",
       "\n",
       "      number_emergency  number_inpatient diag_1  diag_2 diag_3  \\\n",
       "0                    0                 0    311     401    244   \n",
       "1                    0                 1    428     411    284   \n",
       "2                    0                 0    493     401    250   \n",
       "3                    0                 0    552     682    250   \n",
       "4                    0                 0    434     433    401   \n",
       "...                ...               ...    ...     ...    ...   \n",
       "1995                 0                 0    715     401    272   \n",
       "1996                 0                 0    436  250.02    V53   \n",
       "1997                 0                 1     38     585    536   \n",
       "1998                 0                 3    535     276    599   \n",
       "1999                 0                 1    428     403    427   \n",
       "\n",
       "      number_diagnoses  time_in_hospital  \n",
       "0                    7                 7  \n",
       "1                    8                 8  \n",
       "2                    6                 3  \n",
       "3                    3                 4  \n",
       "4                    5                 7  \n",
       "...                ...               ...  \n",
       "1995                 7                 3  \n",
       "1996                 9                 8  \n",
       "1997                 6                13  \n",
       "1998                 9                 2  \n",
       "1999                 9                 8  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "97470adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = ['diag_1','diag_2','diag_3']\n",
    "\n",
    "for i in diags:\n",
    "    for x in range(2000):\n",
    "        a = num_test[i].iloc[x]\n",
    "        if a != None:\n",
    "            if str(a)[0]=='V'or str(a)[0]=='E':\n",
    "                num_test.loc[x,i]= None\n",
    "            else:\n",
    "                num_test.loc[x,i] = float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a701cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d1f2cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\2563865748.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_test[x] = num_test[x].fillna(m)\n",
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\2563865748.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_test[x] = num_test[x].fillna(m)\n",
      "C:\\Users\\johnk\\AppData\\Local\\Temp\\ipykernel_8920\\2563865748.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_test[x] = num_test[x].fillna(m)\n"
     ]
    }
   ],
   "source": [
    "for x in diags:\n",
    "    m = num_test[x].median()\n",
    "    num_test[x] = num_test[x].fillna(m)\n",
    "num_test = np.array(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a595a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['diag_1_desc'] = df_t['diag_1_desc'].fillna('A')\n",
    "df_t['diag_2_desc'] = df_t['diag_2_desc'].fillna('jin')\n",
    "df_t['diag_3_desc'] = df_t['diag_3_desc'].fillna('zhiyuan')\n",
    "text_score_d1 = text_score_calculator(df_t, -3,v_d1,ts_d1)\n",
    "text_score_d2 = text_score_calculator(df_t, -2,v_d2,ts_d2)\n",
    "text_score_d3 = text_score_calculator(df_t, -1,v_d3,ts_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8424601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = np.array([text_score_d1,text_score_d2,text_score_d3])\n",
    "text_test = np.transpose(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "55f55f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 118)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "31752f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "862a6f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate((temp_cate_np[:8000],num_np, text_np),axis = 1)\n",
    "y_train = df['readmitted']\n",
    "test = np.concatenate((cate_test,num_test, text_test),axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1572527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 118)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(temp_cate_np[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b34106c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 132)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc961c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 132)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "38810241",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = GradientBoostingClassifier()\n",
    "i.fit(x_train,y_train)\n",
    "prediction = i.predict_proba(test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = stacking(AdaBoostClassifier, GradientBoostingClassifier)\n",
    "j.fit(x_train, y_train)\n",
    "prd = j.predict(test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc72dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0312dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "85f04d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_csv('Jin_Zhiyuan_pred2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "260850bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = pd.read_csv('pred2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = pred2['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(prediction,pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98441d",
   "metadata": {},
   "source": [
    "### generanl stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f6b03",
   "metadata": {},
   "source": [
    "Now I want to apply stacking not only for the text data, I want to use 3 different types of model to predict data, and use the predictions as three different features to make final prediction. this is the way how I thought the genernal stacking should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level0_predictor(t_train,l_train, model):\n",
    "    leng = int(len(t_train)/2)\n",
    "    t_a, t_b= t_train[:leng], t_train[leng:]\n",
    "    tg_a, tg_b = l_train[:leng], l_train[leng:]\n",
    "    if model == LogisticRegression:\n",
    "        m1 = model(max_iter = 10**8)\n",
    "        m2 = model(max_iter = 10**8)\n",
    "    else:\n",
    "        m1 = model()\n",
    "        m2 = model()\n",
    "    m1.fit(t_a,tg_a)\n",
    "    tp_b = [i[1] for i in m1.predict_proba(t_b)]\n",
    "\n",
    "    m2.fit(t_b,tg_b)\n",
    "    tp_a = [i[1] for i in m2.predict_proba(t_a)]\n",
    "    \n",
    "    tp = [[x] for x in np.hstack((tp_a,tp_b))]\n",
    "    \n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking0(x,y,m_list):\n",
    "    y = np.array(y).astype(float)\n",
    "    meta1 = level0_predictor(x,y,m_list[0])\n",
    "    meta2 = level0_predictor(x,y,m_list[1])\n",
    "    meta3 = level0_predictor(x,y,m_list[2])\n",
    "    output = np.concatenate((meta1, meta2, meta3),axis = 1)\n",
    "    m_3 = []\n",
    "    # m_3 is the output model for the new data set which fed with the entire training set\n",
    "    for t in m_list:\n",
    "        if t==LogisticRegression:\n",
    "            m = t(max_iter = 10**9)\n",
    "        else:\n",
    "            m = t()\n",
    "        m.fit(x,y)\n",
    "        m_3.append(m)\n",
    "    return output,m_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to transform the outsample text data to leval0 prediction\n",
    "# which is predicted by the output model from the function train_stacking\n",
    "def test_stacking0(x,m_list):\n",
    "    prediction1 = [[x[1]] for x in m_list[0].predict_proba(x)]\n",
    "    prediction2 = [[x[1]] for x in m_list[1].predict_proba(x)]\n",
    "    prediction3 = [[x[1]] for x in m_list[2].predict_proba(x)]\n",
    "    prediction = np.concatenate((prediction1, prediction2, prediction3),axis=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacking0:\n",
    "    def __init__(self, l1_model_list, l2_model, **l2_args):\n",
    "        self._l1_predictors = None\n",
    "        self._stacking_model = l1_model_list\n",
    "        if l2_model==LogisticRegression:\n",
    "            self._l2_predictor = l2_model(max_iter = 10**5,solver = l2_args['solver'], penalty = l2_args['penalty'])\n",
    "        elif l2_model==GradientBoostingClassifier:\n",
    "            self._l2_predictor = l2_model(learning_rate = l2_args['learning_rate'],max_depth=l2_args['max_depth'])\n",
    "        else:\n",
    "            self._l2_predictor = l2_model()\n",
    "    def fit(self, X_train, Y_train):\n",
    "        output = train_stacking0(X_train,Y_train,self._stacking_model)\n",
    "        self._l1_predictors = output[1]\n",
    "        self._l2_predictor.fit(output[0],Y_train)\n",
    "    def predict_proba(self, x_test):\n",
    "        l1_prediction = test_stacking0(x_test, self._l1_predictors)\n",
    "        l2_prediction = self._l2_predictor.predict_proba(l1_prediction)\n",
    "        return [i[1] for i in l2_prediction]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001840ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold_stacking0(folds, data, labels, error_function, model1_list,model2, **model_args):\n",
    "    kf = KFold(n_splits=folds, random_state=None, shuffle = True)\n",
    "    scores = []\n",
    "    y = pd.DataFrame(labels)\n",
    "    data = pd.DataFrame(data)\n",
    "    M = stacking0(model1_list, model2,**model_args)\n",
    "    for train_index,test_index in kf.split(data):\n",
    "        x_training_set = data.iloc[train_index]\n",
    "        y_training_set = y.iloc[train_index]\n",
    "        x_test_set = data.iloc[test_index]\n",
    "        y_test_set = y.iloc[test_index]\n",
    "        y_training_set = [x[0] for x in np.array(y_training_set)]\n",
    "        M.fit(np.array(x_training_set), y_training_set)\n",
    "        y_pred = M.predict_proba(np.array(x_test_set))\n",
    "        score = error_function(y_test_set,y_pred)\n",
    "        scores.append(score)\n",
    "        print('.',end='')\n",
    "    average_error = round(sum(scores)/folds,4)\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = np.concatenate((cate_np,num_np,text_np),axis = 1)\n",
    "x_train, x_test, y_train, y_test = partition(con, df['readmitted'],0.2)\n",
    "model_list = [LogisticRegression,GaussianNB, KNeighborsClassifier]\n",
    "training_meta,m_3 = train_stacking0(x_train,y_train,model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42377828",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = stacking0(model_list,GradientBoostingClassifier, learning_rate = 0.7,max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf231efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c702f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p= i.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbe410",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list1 = [LogisticRegression,RandomForestClassifier, AdaBoostClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list2 = [KNeighborsClassifier,RandomForestClassifier, AdaBoostClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sFold_stacking0(5, x_train, y_train, roc_auc_score,model_list1, GradientBoostingClassifier,learning_rate =0.17,max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f539508",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readmitted'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32328c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_train,cate_test,y_train,y_test = partition(cate_np, df['readmitted'],0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55344061",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de053a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in models:\n",
    "    s1 = sFold(5, cate_train, y_train, roc_auc_score , x)\n",
    "    print(x.__name__, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train,num_test,y_train, y_test = partition(num_np,df['readmitted'],0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b22c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in models:\n",
    "    s1 = sFold(5, num_train, y_train, roc_auc_score , x)\n",
    "    print(x.__name__, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = partition(text_np, df['readmitted'],0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in models:\n",
    "    s1 = sFold(5, text_train, y_train, roc_auc_score , x)\n",
    "    print(x.__name__, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0c228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ee1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.permutation(5).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff534261",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.remove(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9400982",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0 for x in range(5)])\n",
    "w[0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d653f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55e9165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    h = np.random.permutation(5).tolist()\n",
    "    for y in h:\n",
    "        if w[y]>0:\n",
    "            j = y\n",
    "            h.remove(j)\n",
    "            i = h[0]\n",
    "    print(w[i],w[j])\n",
    "    w[i]+=0.01\n",
    "    w[j]-=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d042ec1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd05d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
